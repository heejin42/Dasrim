from flask import Flask, request
from flask import json
from flask import jsonify
from flask_cors import CORS
from konlpy.tag import Okt
from konlpy.tag import Kkma
from keras import models
from gensim.models import doc2vec
from gensim.models.doc2vec import TaggedDocument

import pandas as pd
import numpy as np
import jpype
import re
import os
import time

app = Flask (__name__)
CORS(app)

cog_error=''
user_data = ['','']
finish=False
finish_=False

# --------------------- BiLSTM -----------------------

# KoNLPy í˜•íƒœì†Œë¶„ì„ê¸° ì„¤ì •
tagger = Okt()
# ì‚¬ì „ ì •ì˜
PAD = "<PADDING>"
PAD_INDEX = 0
OOV = "<OOV>"
OOV_INDEX = 1

# í˜•íƒœì†Œë¶„ì„ í•¨ìˆ˜
def pos_tag(sentences):
    
    # ë¬¸ì¥ í’ˆì‚¬ ë³€ìˆ˜ ì´ˆê¸°í™”
    sentences_pos = []
    
    # ì¸í’‹ì´ ë¦¬ìŠ¤íŠ¸ë©´
    if isinstance(sentences,list):
    # ëª¨ë“  ë¬¸ì¥ ë°˜ë³µ
        for sentence in sentences:
            # [\"':;~()] íŠ¹ìˆ˜ê¸°í˜¸ ì œê±°
            sentence = re.sub("[ã„±-ã…ã…-ã…£-=+,#/\?:^$.@*\"â€»~&%ã†!ã€\\â€˜|\(\)\[\]\<\>`\'â€¦ã€‹]", " ", sentence)
            
            
            # ë°°ì—´ì¸ í˜•íƒœì†Œë¶„ì„ì˜ ì¶œë ¥ì„ ë„ì–´ì“°ê¸°ë¡œ êµ¬ë¶„í•˜ì—¬ ë¶™ì„
            sentence = " ".join(tagger.morphs(sentence))
            sentences_pos.append(sentence)
            
    # strì´ë©´        
    elif isinstance(sentences, str):
        sentences=re.sub("[ã„±-ã…ã…-ã…£-=+,#/\?:^$.@*\"â€»~&%ã†!ã€\\â€˜|\(\)\[\]\<\>`\'â€¦ã€‹]", " ", sentences)
        sentences_pos= " ".join(tagger.morphs(sentences))
        
    return sentences_pos

# ì •ìˆ˜ ì¸ì½”ë”© ë° íŒ¨ë”©
def convert_text_to_index_for_classification(sentences, vocabulary): 
    sentences_index = []
    if isinstance(sentences,list):
        # ëª¨ë“  ë¬¸ì¥ì— ëŒ€í•´ì„œ ë°˜ë³µ
        for sentence in sentences:
            sentence_index = []

            # ë¬¸ì¥ì˜ ë‹¨ì–´ë“¤ì„ ë„ì–´ì“°ê¸°ë¡œ ë¶„ë¦¬
            for word in sentence.split():
                if vocabulary.get(word) is not None:
                    # ì‚¬ì „ì— ìˆëŠ” ë‹¨ì–´ë©´ í•´ë‹¹ ì¸ë±ìŠ¤ë¥¼ ì¶”ê°€
                    sentence_index.extend([vocabulary[word]])
                else:
                    # ì‚¬ì „ì— ì—†ëŠ” ë‹¨ì–´ë©´ OOV ì¸ë±ìŠ¤ë¥¼ ì¶”ê°€
                    sentence_index.extend([vocabulary[OOV]])

            if len(sentence_index) > max_sequences:
                sentence_index = sentence_index[:max_sequences]

            # ìµœëŒ€ ê¸¸ì´ì— ì—†ëŠ” ê³µê°„ì€ íŒ¨ë”© ì¸ë±ìŠ¤ë¡œ ì±„ì›€
            sentence_index += (max_sequences - len(sentence_index)) * [vocabulary[PAD]]

            # ë¬¸ì¥ì˜ ì¸ë±ìŠ¤ ë°°ì—´ì„ ì¶”ê°€
            sentences_index.append(sentence_index)

    elif isinstance(sentences, str):
        sentence_index = []
        for word in sentences.split():
            if vocabulary.get(word) is not None:
                # ì‚¬ì „ì— ìˆëŠ” ë‹¨ì–´ë©´ í•´ë‹¹ ì¸ë±ìŠ¤ë¥¼ ì¶”ê°€
                sentence_index.extend([vocabulary[word]])
            else:
                # ì‚¬ì „ì— ì—†ëŠ” ë‹¨ì–´ë©´ OOV ì¸ë±ìŠ¤ë¥¼ ì¶”ê°€
                sentence_index.extend([vocabulary[OOV]])

        if len(sentence_index) > max_sequences:
            sentence_index = sentence_index[:max_sequences]

        # ìµœëŒ€ ê¸¸ì´ì— ì—†ëŠ” ê³µê°„ì€ íŒ¨ë”© ì¸ë±ìŠ¤ë¡œ ì±„ì›€
        sentence_index += (max_sequences - len(sentence_index)) * [vocabulary[PAD]]

        sentences_index.append(sentence_index)
    return sentences_index


# íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸°
df_main = pd.read_csv('sentences.csv',encoding='utf-8')
a=df_main['Emotion'].unique()
category =list(a)
CATEGORY = len(category)

# ë¶„ë…¸ 0,ìŠ¬í”” 1, ì¤‘ë¦½ 2, í–‰ë³µ 3,
# ì¹´í…Œê³ ë¦¬ ì¸ë±ìŠ¤

category_to_index = {word: index for index, word in enumerate(category)}

index_to_category = {index: word for index, word in enumerate(category)}

words = []
ori_sentence =[]
# ë°ì´í„° í”„ë ˆì„ listí™”
for i in range(len(df_main)):
    tmp =[]
    tmp.append(str(i+1))
    tmp.append(df_main.iloc[i].Sentence)
    tmp.append(df_main.iloc[i].Emotion)
    ori_sentence.append(tmp)
    
# ë¬¸ì¥ í˜•íƒœì†Œ ë¶„ì„ ë° ì „ì²˜ë¦¬
sente=[]
for i in ori_sentence:
    sente.append(i[1])

senten = pos_tag(sente)

# ë‹¨ì–´ë“¤ì˜ ë°°ì—´ ìƒì„±
for sentence in senten:
    for word in sentence.split():
        words.append(word)

# ê¸¸ì´ê°€ 0ì¸ ë‹¨ì–´ëŠ” ì‚­ì œ
words = [word for word in words if len(word) > 0]

# ì¤‘ë³µëœ ë‹¨ì–´ ì‚­ì œ
words = list(set(words))

# ì œì¼ ì•ì— íƒœê·¸ ë‹¨ì–´ ì‚½ì…
words[:0] = [PAD, OOV]

VOCAB_SIZE = len(words)
# print(VOCAB_SIZE)

# ë¬¸ì¥ ê¸¸ì´ í™•ì¸
max_length = max(len(l) for l in senten)
avg_length = sum(map(len, senten))/len(senten)
# print('ë¦¬ë·°ì˜ ìµœëŒ€ ê¸¸ì´ : {}'.format(max_length))
# print('ë¦¬ë·°ì˜ í‰ê·  ê¸¸ì´ : {}'.format(avg_length))
max_sequences= int(avg_length) + 15

# ë‹¨ì–´ì™€ ì¸ë±ìŠ¤ì˜ ë”•ì…”ë„ˆë¦¬ ìƒì„±
word_to_index = {word: index for index, word in enumerate(words)}
index_to_word = {index: word for index, word in enumerate(words)}

# ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°
load_c_model = models.load_model('main_lstm_cl.h5')

# ëª¨ë¸ì— ì‚¬ìš©í•  í•¨ìˆ˜ ì •ì˜
    
def predict_emotion(text):
    pre_text = pos_tag(text)
    pre_x=convert_text_to_index_for_classification(pre_text, word_to_index)
    result=np.argmax(load_c_model.predict(np.asarray(pre_x).reshape(1,max_sequences)))
    return index_to_category[result]

# --------------------- Doc 2 Vec -----------------------

# íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸°
df = pd.read_csv('ChatbotData.csv')
df_Q=df['Q']
df_A=df['A']

# Series -> list
list_Q_A=[]
for i in range(len(df_Q)):
    temp=[]
    temp.append(str(i+1))
    temp.append(df_Q.iloc[i])
    temp.append(df_A.iloc[i])
    
    list_Q_A.append(temp)

# í˜•íƒœì†Œ ë¶„ì„
kkma = Kkma()
filter_kkma = ['NNG', 'NNP','OL','VA','VV','VXV']

def tokenizer_kkma(doc):
#     jpype.attachThreadToJVM()       
    token_doc = ["/".join(word) for word in kkma.pos(doc)]
    return token_doc

def tokenize_kkma_noun_verb(doc):
#     jpype.attachThreadToJVM()
    token_doc = ["/".join(word) for word in kkma.pos(doc) if word[1] in filter_kkma]
    return token_doc

# tokení™” í›„ ì¸ë±ì‹± ë° ë¬¸ì„œ íƒœê¹…
token_Q_A = [(tokenizer_kkma(row[1]), row[0]) for row in list_Q_A]
tagged_Q_A = [TaggedDocument(d,[c]) for d,c in token_Q_A]

# ëª¨ë¸ ë§Œë“¤ê¸°
import multiprocessing
# ë‚´ ì»´ì— ìˆëŠ” cpu ê°¯ìˆ˜ cores ì— ì €ì¥
cores = multiprocessing.cpu_count()
# vector_size : ì„ë² ë”©í•  ë²¡í„° ì°¨ì›
# negaive : negative sampling
d2v_Q_A = doc2vec.Doc2Vec(
    vector_size = 100,
    alpha = 0.025,
    min_alpha = 0.025,
    hs = 1,
    negative = 0,
    dm = 0,
    dbow_words = 1,
    min_count = 1,
    workers = cores,
    seed = 0
)

# ë‹¨ì–´ ì‚¬ì „ ë§Œë“¤ê¸°
d2v_Q_A.build_vocab(tagged_Q_A)

# ëª¨ë¸ í•™ìŠµ
for epoch in range(4):
    print(f'{epoch} íšŒ í•™ìŠµ ì¤‘...............')
    d2v_Q_A.train(tagged_Q_A,
                    total_examples = d2v_Q_A.corpus_count,
                    epochs = d2v_Q_A.epochs)
    d2v_Q_A.alpha -=0.0025
    d2v_Q_A.min_alpha = d2v_Q_A.min_alpha

# ëª¨ë¸ ì˜ˆì¸¡

def doc2_answer(input_question):
    token_test = tokenizer_kkma(input_question)
    predict_vector = d2v_Q_A.infer_vector(token_test)
    result = d2v_Q_A.docvecs.most_similar([predict_vector],topn=1)
    return list_Q_A[int(result[0][0])-1][2]


@app.route('/generateQ',methods=['POST'])
def generate_Question():

    # ì™¸ë¶€ ë³€ìˆ˜ ì •ì˜
    global user_data # ìƒí™©, ìë™ì  ì‚¬ê³ , ê°ì •ì„ ë‹´ì€ ë°°ì—´
    global finish # ë§ˆì§€ë§‰ì¸ì§€ í™•ì¸
    global finish_ # ë§ˆë¬´ë¦¬ì¸ì§€ í™•ì¸
    global cog_error # ì¸ì§€ì  ì˜¤ë¥˜

    #json ë°ì´í„°ë¥¼ ë°›ì•„ì˜´
    
    user_A = request.form['text1']
    user_name = request.form['user']
    index = int(request.form['idx'])
    user_cnt=request.form['user_count']

    # ì¸ì§€ì  ì˜¤ë¥˜ A ë¦¬ìŠ¤íŠ¸ ì¸ì§€ì ì˜¤ë¥˜ : [ì„¤ëª…,ëŒ€ë‹µ]
    cognitive_error={
        'ì‚¬ê³ ì˜ í™•ëŒ€ì™€ ì¶•ì†Œ':['ìì‹ ì˜ ì¢‹ì§€ ì•Šì€ ë©´ì€ í™•ëŒ€í•˜ê³  ì¢‹ì€ ë©´ì€ ì¶•ì†Œí•˜ëŠ” ê²½ìš°',
        f'ë„ˆë¬´ ìì‹ ì„ ê³¼ì†Œí‰ê°€í•˜ëŠ” ê²ƒì´ ì•„ë‹Œì§€ ìƒê°í•´ë³´ì„¸ìš”. {user_name}ë‹˜ì€ ì´ë¯¸ ì¶©ë¶„íˆ ì˜í•˜ê³  ìˆëŠ”ê±¸ìš”!'],
        '':['','']
    }

    # ì±—ë´‡ ê³ ì • ì§ˆë¬¸ ë¦¬ìŠ¤íŠ¸
    bot_a_list = {
        'ê³ ì •':['ì™œ ê·¸ëŸ° ìƒê°ì´ ë“œì…¨ëŠ”ì§€ ì„¤ëª…í•´ ì£¼ì‹¤ ìˆ˜ ìˆë‚˜ìš”?',
                f'ìµœê·¼ì— {user_name}ë‹˜ì´ ë³¸ì¸ì„ ê·¸ë ‡ê²Œ ëŠë‚„ë§Œí•œ ì¼ë“¤ì´ ìˆì—ˆë‚˜ìš”?',#ìƒí™©
                'ê·¸ëŸ° ìƒí™©ì—ì„œ ìŠ¤ì³ì§€ë‚˜ê°„ ìƒê°ì´ ìˆìœ¼ì‹ ê°€ìš”?',# ìë™ì  ì‚¬ê³ 
                f'ê·¸ ë•Œ ëŠë‚€ ê°ì •ì€ ì–´ë• ë‚˜ìš”?', # ê°ì •
                f"ê·¸ëŸ¼ '{user_data[1]}' ë¼ëŠ” ë§ì€ ë³¸ì¸ì—ê²Œ ë¬´ì—‡ì„ 'ì˜ë¯¸'í•˜ë‚˜ìš”?"],
        'ë¶€ì •':['ê·¸ë ‡ë‹¤ë©´ ê·¸ê²ƒì„ ê°œì„ í•˜ê¸° ìœ„í•´ì„œëŠ” ì–´ë–»ê²Œ í•´ì•¼ í•˜ë‚˜ìš”?',
                'ê·¸ë ‡ê²Œ ë˜ì§€ ì•ŠëŠ”ë‹¤ë©´ ìµœì•…ì˜ ê²½ìš°ëŠ” ì–´ë–»ê²Œ ë ê¹Œìš”?'],#í•µì‹¬ì‹ ë…
        'ê¸ì •':'ê·¸ë ‡ê²Œ ìƒê°í•˜ëŠ” ì´ìœ ê°€ ìˆë‹¤ë©´ ì–´ë–¤ê²ƒì´ì£ ?', #í•µì‹¬ì‹ ë…
        'ë§ˆë¬´ë¦¬' : ["ê·¸ë ‡êµ°ìš”.. ì§€ê¸ˆê¹Œì§€ ì„±ì‹¤íˆ ë‹µí•´ì£¼ì…”ì„œ ê°ì‚¬í•´ìš”ğŸ˜Œ",f"ì˜¤ëŠ˜ {user_cnt}íšŒê¸° ìƒë‹´ì€ ì—¬ê¸°ì„œ ë§ˆë¬´ë¦¬ í•˜ê² ìŠµë‹ˆë‹¤. ì–´ë•Œìš”! {user_name}ë‹˜ ë³¸ì¸ì˜ ìƒê°ì— ëŒ€í•´ ê¹Šê²Œ ìƒê°í•´ë³´ê²Œ ëœ ê²ƒ ê°™ë‚˜ìš”?!"],
        'ì¢…ë£Œ':[f'ì €ëŠ” {user_name}ë‹˜ì„ ì•Œ ìˆ˜ ìˆëŠ” ì¢‹ì€ ì‹œê°„ì´ ë˜ì—ˆì–´ìš”!. ì˜¤ëŠ˜ ëŒ€í™” ì¤‘ "{user_data[1]}" ë¼ê³  ë§ì”€í•˜ì…¨ëŠ”ë° ì œê°€ ìƒê°í•˜ê¸°ì—ëŠ” ê·¸ê±´ ì¸ì§€ì  ì˜¤ë¥˜ ì¤‘ "{cog_error}"ì— í•´ë‹¹í•˜ëŠ” ê²ƒ ê°™ì•„ìš”. ì´ëŠ” {cognitive_error[cog_error][0]}ì— . {cognitive_error[cog_error][1]}',
                f'ì €ëŠ” {user_name}ë‹˜ì„ ì•Œ ìˆ˜ ìˆëŠ” ì¢‹ì€ ì‹œê°„ì´ ë˜ì—ˆì–´ìš”!. ì˜¤ëŠ˜ ëŒ€í™” ì¤‘ "{user_data[1]}" ë¼ê³  ë§ì”€í•˜ì…¨ëŠ”ë° ì´ë ‡ê²Œ ìƒê° í•˜ì‹  ê²ƒì— ê³¼ë„í•œ ì¼ë°˜í™”, í‘ë°±ë…¼ë¦¬ ë“±ê³¼ ê°™ì€ ì¸ì§€ì  ì˜¤ë¥˜ê°€ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤! í‰ì†Œ ìŠ¤ì²˜ê°€ëŠ” ìƒê°ì— ëŒ€í•´ ë‹¤ì‹œ í•œë²ˆ ê¹Šê²Œ ìƒê°ì„ í•´ë³´ì…¨ìœ¼ë©´ í•´ìš”!',
                f'ì•ì„œ "{user_data[0]}"ë¼ëŠ” ìƒí™©ì¼ ë•Œ "{user_data[1]}"ë¼ëŠ” ìƒê°ì²˜ëŸ¼ ì£¼ì–´ì§„ ìƒí™© ì†ì— ì¦‰ê°ì ìœ¼ë¡œ ëŠë¼ëŠ” ìƒê°ì„ ë§í•©ë‹ˆë‹¤. ì¸ì§€í–‰ë™ì¹˜ë£ŒëŠ” {user_name}ë‹˜ì˜ ğŸ”¥ì ê·¹ì ì¸ ì°¸ì—¬ğŸ”¥ê°€ í•„ìš”í•´ìš”. ì œê°€ ë°©ê¸ˆ ìƒë‹´ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ "ìë™ì  ì‚¬ê³  ê¸°ë¡ì§€"ë¥¼ í•œ ì¤„ ì ì—ˆìœ¼ë‹ˆ ì°¸ê³ í•´ì„œ ë‹¤ìŒ ìƒë‹´ ë•Œê¹Œì§€ 5ê°œë¥¼ ê¼¬ì˜¥~!ğŸ¤™ ì‘ì„±í•´ì£¼ì„¸ìš”~ğŸ˜‰'],
        'ëë‚´ê¸°' : 'ëë‚´ì‹œê² ë‹¤ë‹ˆ ì•Œê² ì–´ìš”..ğŸ˜¥ ì–¸ì œë¼ë„ ì œê°€ í•„ìš”í•˜ë©´ ì°¾ì•„ì™€ì£¼ì„¸ìš”~ ğŸ‘‹'
                }
    # ê°ì •ì— ë”°ë¥¸ ë¦¬ì•¡ì…˜ A ë¦¬ìŠ¤íŠ¸
    bot_a_emotion={'í–‰ë³µ':'ì™€ ë„ˆë¬´ ì¢‹ìœ¼ì…¨ê² ì–´ìš”!! ê¸°ì¨ì€ ë‚˜ëˆ„ë©´ ë°°ê°€ ëœë°ìš”!! ì œê°€ ì œê³±ìœ¼ë¡œ ëŠ˜ë ¤ë“œë¦´ê»˜ìš”!!!',
                'ë¶„ë…¸':'ëŒ€ë”°ëŒ€ë”° í˜ë“œì…¨ê² ë‹¤.. ì–´ë–»ê²Œ ë²„í‹°ì…¨ì–´ìš”ğŸ˜¥',
                'ì¤‘ë¦½':'ì•„.. ê·¸ëŸ¬ì…¨êµ¬ë‚˜ ì¶©ë¶„íˆ ê·¸ëŸ´ ìˆ˜ ìˆì£ !',
                'ìŠ¬í””':'ë§ˆìŒì•„í”„ì‹œê² ì–´ìš” ..ğŸ˜¢ ìŠ¬í””ì€ ë‚˜ëˆ„ë©´ ë‚˜ëˆ ì§„ë‹¤ê³  í–ˆì–´ìš”..!',
                'ê³µê°':f'ë§ì•„ìš” ì œ ìƒê°ë„ ê·¸ë˜ìš”. {user_name}ë‹˜ì€ ì¶©ë¶„íˆ ë…¸ë ¥í•˜ì…¨ì–´ìš”.'} 
    bot_Q_list=[]
    
    # ìœ ì €ê°€ ì›í•˜ë©´ ì¢…ë£Œ
    if user_A == 'ëë‚´ê¸°':
        bot_Q_list.append(bot_a_list['ëë‚´ê¸°'])
        response={
            'bot_Q':bot_Q_list,
            'end_talk': 'end_talk'
        }
        return jsonify(response)
    elif index == -1:
        bot_Q_list.append('')
    # ë§ˆì§€ë§‰ ì „ ì´ë©´
    elif index < len(bot_a_list['ê³ ì •']):
        
        # ìƒí™© ì§ˆë¬¸ ë’¤ ì‚¬ìš©ìì˜ ë‹µë³€ ìƒí™©ì— ì €ì¥ ë° ê°ì •ì— ë§ëŠ” ë¦¬ì•¡ì…˜ ì „ë‹¬
        if index==2 : 
            user_data.append(user_A)
            emotion = predict_emotion(user_A)
            bot_Q_list.append(bot_a_emotion[emotion])
            bot_Q_list.append(bot_a_list['ê³ ì •'][index])
            del user_data[0]

        # ìë™ì  ì‚¬ê³  ì§ˆë¬¸ ë’¤ ì‚¬ìš©ìì˜ ë‹µë³€ ìì‚¬ì— ì €ì¥
        elif index==3 : 
            user_data.append(user_A)
            del user_data[0]
            bot_Q_list.append(f'ë°©ê¸ˆ ë§í•œ "{user_data[1]}" ì™€ ê°™ì€ ìƒê°ì„ í–ˆì„ ë•Œ ëŠë‚€ "ê°ì •"ì€ ì–´ë–¤ê°€ìš”?')

        # ê°ì • ì§ˆë¬¸ ë’¤ ì‚¬ìš©ìì˜ ë‹µë³€ ê°ì •ì— ì €ì¥ ë° ì¸ì§€ì  ì˜¤ë¥˜ ì €ì¥
        elif index==4 : 
            user_data.append(user_A)
            cog_error = doc2_answer(user_data[1])
            print('doc2vec ê²°ê³¼ : ',doc2_answer(user_data[1]))

            if cog_error in cognitive_error:
                ans = cog_error+'('+ cognitive_error[cog_error][0]+')'
                user_data.append(ans)
            else :
                user_data.append('')
                cog_error = ''
            bot_Q_list.append(bot_a_list['ê³ ì •'][index])

        else :
            bot_Q_list.append(bot_a_list['ê³ ì •'][index])

    # ë§ˆì§€ë§‰ ê³ ì • ë©˜íŠ¸ì‹œ ìœ ì € ê°ì • ë¶„ì„
    elif index == 5 :
        emotion=predict_emotion(user_A)
        if emotion == 'ê¸ì •':
            bot_Q_list.append(bot_a_list['ê¸ì •'])
            finish_ = True
        else :
            bot_Q_list.append(bot_a_list['ë¶€ì •'][0])

    # ëë‚¬ì„ ë•Œ       
    elif finish :
        if cog_error == '':
            bot_Q_list.append(bot_a_list['ì¢…ë£Œ'][1])
            bot_Q_list.append(bot_a_list['ì¢…ë£Œ'][2])
        else : 
            bot_Q_list.append(bot_a_list['ì¢…ë£Œ'][0])
            bot_Q_list.append(bot_a_list['ì¢…ë£Œ'][2])
        
        response={
            'bot_Q':bot_Q_list,
            # user_data ìƒí™©, ìì‚¬, ê°ì •, ì¸ì§€ì ì˜¤ë¥˜, í•µì‹¬ì‹ ë… ìˆœì„œ
            'user_data':user_data,
            'talk_finish':'finish'
        }
        user_data = ['','']
        cog_error=''
        finish = False
        finish_ = False
        time.sleep(3)
        
        return jsonify(response)

    elif finish_:
        user_data.append(user_A)
        bot_Q_list.append(bot_a_list['ë§ˆë¬´ë¦¬'][0])
        bot_Q_list.append(bot_a_list['ë§ˆë¬´ë¦¬'][1])
        finish = True

    # ë¶€ì •ì¼ ë•Œ
    else :
        bot_Q_list.append(bot_a_list['ë¶€ì •'][1])
        finish_=True
      
        



    # ì§ˆë¬¸ 3ì´ˆ ì§€ì—°
    time.sleep(3)
    
    response={'bot_Q':bot_Q_list}

    return jsonify(response)


if __name__ == "__main__":
    print("I'm ready!")
    app.run(host='192.168.0.163', port='5001', debug=True)